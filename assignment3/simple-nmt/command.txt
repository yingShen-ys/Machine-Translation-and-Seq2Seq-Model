# use it with original repo
python train.py \
    --model ./models/deen.pth \
    --train ./data/iwslt/corpus_xnli.train \
    --valid ./data/iwslt/corpus.valid \
    --lang deen \
    --adam \
    --lr 0.0001 \
    --word_vec_dim 256 \
    --hidden_size 512 \
    --batch_size 64 \
    --n_epochs 15 \
    --rl_n_epochs -1 \
    --early_stop -1 \
    --n_layers 2 \
    --batch_size 64 \
    --pretrain \
    --gpu_id -1

# Use reinforce
python train.py \
    --n_epochs -1 \
    --model merged_vocab_best.pth \
    --bimpm_pretrained_model_path BIMPM_best_merged_vocab.pt \
    --train data/nli_split/nli.train \
    --valid data/nli_split/nli.valid \
    --lang deen \
    --print_every 40 \
    --rl_n_gram 4 \
    --rl_lr 0.001 \
    --rl_n_epochs 15 \
    --gpu_id -1

# Use minimum risk training and only bleu reward for validation
python train.py \
    --n_epochs -1 \
    --model deen.iwslt_xnli_best.pth \
    --bimpm_pretrained_model_path BIMPM_multinli_iwslt_merged_vocab_best.pt \
    --train data/nli_split/nli.train \
    --valid data/iwslt/corpus.valid \
    --valid_nli data/nli_iwslt/nli.all \
    --lang deen \
    --print_every 40 \
    --rl_n_gram 4 \
    --rl_lr 0.05 \
    --rl_n_epochs 15 \
    --n_samples 5 \
    --use_minimum_risk \
    --temperature 1.0 \
    --reward_mode bleu \
    --pretrain \
    --batch_size 32 \
    --max_grad_norm 0.1 \
    --gpu_id -1

# Use minimum risk training and combined losses
python train.py \
    --n_epochs -1 \
    --model deen.iwslt_xnli_best.pth \
    --bimpm_pretrained_model_path BIMPM_multinli_iwslt_merged_vocab_best.pt \
    --train data/nli_split/nli.train \
    --valid data/iwslt/corpus.valid \
    --valid_nli data/nli_split/nli.valid \
    --lang deen \
    --print_every 40 \
    --rl_n_gram 4 \
    --rl_lr 0.05 \
    --rl_n_epochs 10 \
    --n_samples 2 \
    --use_minimum_risk \
    --temperature 1.0 \
    --reward_mode combined \
    --batch_size 32 \
    --max_grad_norm 0.1 \
    --gpu_id -1

python translate.py --model deen.iwslt_xnli_best.pth --input_file ./data/nli_split/nli.train.de --output_file nli.train.en.translated